from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, FileResponse
import os
from dotenv import load_dotenv
import asyncio
from datetime import datetime
import json
from faster_whisper import WhisperModel
from mistralai import Mistral
from typing import Dict, List, Any, Optional
import traceback
import librosa
import soundfile as sf
import numpy as np
from pyannote.audio import Pipeline
import torch
from pydub import AudioSegment

# Deepgram imports (optional - handle compatibility issues)
DEEPGRAM_AVAILABLE = False
try:
    from deepgram import DeepgramClient, PrerecordedOptions, FileSource
    DEEPGRAM_AVAILABLE = True
    print("‚úÖ Deepgram SDK available")
except ImportError:
    print("‚ö†Ô∏è  Deepgram SDK not installed, using Faster-Whisper only")
except SyntaxError:
    print("‚ö†Ô∏è  Deepgram SDK incompatible with Python 3.9, using Faster-Whisper only")
except Exception as e:
    print(f"‚ö†Ô∏è  Deepgram SDK error: {e}, using Faster-Whisper only")

load_dotenv()

app = FastAPI(title="AI Meeting Transcription - Dual Engine", version="2.0.0")

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.on_event("startup")
async def startup_event():
    """Initialize models on startup"""
    print("üîÑ Initializing AI models on startup...")
    load_models()
    print("‚úÖ Startup initialization complete!")

# Global variables
whisper_model = None
mistral_client = None
diarization_pipeline = None
deepgram_client = None
processing_jobs = {}

# Configuration
TRANSCRIPTION_ENGINE = os.getenv("TRANSCRIPTION_ENGINE", "faster-whisper")  # "faster-whisper" or "deepgram"

def load_models():
    """Load AI models with error handling"""
    global whisper_model, mistral_client, diarization_pipeline, deepgram_client
    
    try:
        print(f"üîß Transcription engine: {TRANSCRIPTION_ENGINE}")
        
        # Always try to initialize Deepgram client for availability check
        if deepgram_client is None and DEEPGRAM_AVAILABLE:
            api_key = os.getenv("DEEPGRAM_API_KEY")
            if api_key:
                deepgram_client = DeepgramClient(api_key)
                print("‚úÖ Deepgram client initialized!")
            else:
                print("‚ùå DEEPGRAM_API_KEY not found")
        elif not DEEPGRAM_AVAILABLE:
            print("‚ùå Deepgram SDK not available")
        
        # Load Faster-Whisper if needed or if it's the current engine
        if TRANSCRIPTION_ENGINE == "faster-whisper" or whisper_model is None:
            if whisper_model is None:
                print("Loading Faster-Whisper model (small - High Performance)...")
                whisper_model = WhisperModel("small", device="cpu", compute_type="int8")
                print("‚úÖ Faster-Whisper model loaded!")
        
        # Load Mistral client
        if mistral_client is None:
            api_key = os.getenv("MISTRAL_API_KEY")
            if api_key:
                mistral_client = Mistral(api_key=api_key)
                print("‚úÖ Mistral client initialized!")
            else:
                print("‚ö†Ô∏è  MISTRAL_API_KEY not found")
        
        if diarization_pipeline is None:
            try:
                print("Loading pyannote speaker diarization model...")
                # Try different models in order of preference
                models_to_try = [
                    "pyannote/speaker-diarization-3.1",
                    "pyannote/speaker-diarization-3.0", 
                    "pyannote/speaker-diarization",
                    "pyannote/segmentation-3.0"
                ]
                
                hf_token = os.getenv("HUGGING_FACE_TOKEN")
                print(f"üîë HF Token loaded: {'Yes' if hf_token else 'No'} (length: {len(hf_token) if hf_token else 0})")
                
                for model_name in models_to_try:
                    try:
                        print(f"Trying to load: {model_name}")
                        if hf_token and hf_token != "your_hf_token_here" and hf_token != "GANTI_DENGAN_TOKEN_ANDA_DISINI":
                            # Use both 'token' (newer) and 'use_auth_token' (fallback) parameters
                            try:
                                diarization_pipeline = Pipeline.from_pretrained(
                                    model_name,
                                    token=hf_token
                                )
                                print(f"‚úÖ Speaker diarization model loaded: {model_name} (with token)")
                            except (TypeError, Exception):
                                # Fallback to older parameter name
                                print(f"Trying use_auth_token fallback for {model_name}...")
                                try:
                                    diarization_pipeline = Pipeline.from_pretrained(
                                        model_name,
                                        use_auth_token=hf_token
                                    )
                                    print(f"‚úÖ Speaker diarization model loaded: {model_name} (with use_auth_token)")
                                except Exception as fallback_error:
                                    print(f"‚ö†Ô∏è  Failed to load {model_name} with token: {fallback_error}")
                                    raise fallback_error
                        else:
                            # Try without token (some models are public)
                            diarization_pipeline = Pipeline.from_pretrained(model_name)
                            print(f"‚úÖ Speaker diarization model loaded: {model_name} (public)")
                        
                        # If we reach here, model was loaded successfully
                        break
                        
                    except Exception as model_error:
                        print(f"‚ö†Ô∏è  Failed to load {model_name}: {model_error}")
                        continue
                
                if diarization_pipeline is None:
                    raise Exception("No diarization models could be loaded")
                    
            except Exception as e:
                print(f"‚ö†Ô∏è  Could not load any diarization model: {e}")
                print("‚ÑπÔ∏è  Will implement simple voice activity detection as fallback...")
                diarization_pipeline = "disabled"  # Mark as disabled
                
    except Exception as e:
        print(f"‚ùå Model loading error: {e}")

@app.get("/")
async def root():
    active_engine = TRANSCRIPTION_ENGINE
    if TRANSCRIPTION_ENGINE == "deepgram" and not deepgram_client:
        active_engine = "faster-whisper (fallback)"
    
    return {
        "message": "AI Meeting Transcription - Dual Engine Support", 
        "status": "running",
        "transcription_engine": active_engine,
        "features": ["Faster-Whisper (Local)", "Deepgram (Cloud)", "Mistral AI", "Speaker Diarization"],
        "engines_available": {
            "faster_whisper": whisper_model is not None,
            "deepgram": deepgram_client is not None
        },
        "timestamp": datetime.now().isoformat()
    }

@app.post("/api/upload-and-process")
async def upload_and_process(file: UploadFile = File(...)):
    """Upload and process with librosa instead of FFmpeg"""
    try:
        if not file.filename:
            raise HTTPException(status_code=400, detail="No file uploaded")
        
        content = await file.read()
        if len(content) > 150 * 1024 * 1024:  # 150MB limit
            raise HTTPException(status_code=400, detail="File too large. Maximum 150MB.")
        
        # Check file format
        allowed_extensions = ['.wav', '.mp3', '.m4a', '.flac', '.ogg']
        file_ext = os.path.splitext(file.filename)[1].lower()
        if file_ext not in allowed_extensions:
            raise HTTPException(status_code=400, detail=f"Unsupported format: {file_ext}")
        
        # Generate job ID
        job_id = f"job_{datetime.now().strftime('%Y%m%d_%H%M%S_%f')[:20]}"
        processing_jobs[job_id] = {"status": "starting", "progress": 0, "message": "Initializing..."}
        
        # Save file
        uploads_dir = os.path.join(os.path.dirname(__file__), "uploads")
        os.makedirs(uploads_dir, exist_ok=True)
        file_path = os.path.join(uploads_dir, f"{job_id}{file_ext}")
        
        with open(file_path, 'wb') as f:
            f.write(content)
        
        print(f"üìÅ File saved: {file_path} ({len(content)/1024:.1f} KB)")
        
        # Start processing
        asyncio.create_task(process_audio_librosa(job_id, file_path, file.filename))
        
        return JSONResponse({
            "job_id": job_id,
            "status": "processing_started",
            "message": f"File uploaded ({len(content)/1024:.1f} KB). Using librosa processing.",
            "file_size_kb": len(content)/1024
        })
        
    except Exception as e:
        print(f"‚ùå Upload error: {e}")
        raise HTTPException(status_code=500, detail=f"Upload failed: {str(e)}")

@app.get("/api/status/{job_id}")
async def get_processing_status(job_id: str):
    if job_id not in processing_jobs:
        raise HTTPException(status_code=404, detail="Job not found")
    return processing_jobs[job_id]

@app.get("/api/result/{job_id}")
async def get_result(job_id: str):
    if job_id not in processing_jobs:
        raise HTTPException(status_code=404, detail="Job not found")
    
    job_status = processing_jobs[job_id]
    if job_status["status"] != "completed":
        raise HTTPException(status_code=400, detail=f"Job status: {job_status['status']}")
    
    results_dir = os.path.join(os.path.dirname(__file__), "results")
    result_file = os.path.join(results_dir, f"{job_id}_result.json")
    if not os.path.exists(result_file):
        raise HTTPException(status_code=404, detail="Result file not found")
    
    with open(result_file, 'r', encoding='utf-8') as f:
        result = json.load(f)
    
    return result

@app.get("/api/config")
async def get_config():
    """Get current transcription engine configuration"""
    return {
        "transcription_engine": TRANSCRIPTION_ENGINE,
        "engines_available": {
            "faster_whisper": whisper_model is not None,
            "deepgram": deepgram_client is not None and DEEPGRAM_AVAILABLE
        },
        "deepgram_sdk_available": DEEPGRAM_AVAILABLE,
        "fallback_enabled": True
    }

@app.post("/api/config/engine")
async def set_transcription_engine(engine: str):
    """Set transcription engine (faster-whisper or deepgram)"""
    global TRANSCRIPTION_ENGINE, whisper_model, deepgram_client
    
    if engine not in ["faster-whisper", "deepgram"]:
        raise HTTPException(status_code=400, detail="Invalid engine. Use 'faster-whisper' or 'deepgram'")
    
    # Check if Deepgram is requested but not available
    if engine == "deepgram" and not DEEPGRAM_AVAILABLE:
        raise HTTPException(status_code=400, detail="Deepgram SDK not available. Please install or use faster-whisper")
    
    if engine == "deepgram" and not os.getenv("DEEPGRAM_API_KEY"):
        raise HTTPException(status_code=400, detail="DEEPGRAM_API_KEY not configured")
    
    # Update configuration
    old_engine = TRANSCRIPTION_ENGINE
    TRANSCRIPTION_ENGINE = engine
    
    # Load appropriate models
    try:
        load_models()
        return {
            "status": "success",
            "previous_engine": old_engine,
            "current_engine": TRANSCRIPTION_ENGINE,
            "message": f"Transcription engine switched to {engine}"
        }
    except Exception as e:
        # Rollback on error
        TRANSCRIPTION_ENGINE = old_engine
        raise HTTPException(status_code=500, detail=f"Failed to switch engine: {str(e)}")

@app.get("/api/engines")
async def get_available_engines():
    """Get information about available transcription engines"""
    return {
        "engines": {
            "faster-whisper": {
                "name": "Faster-Whisper",
                "type": "local",
                "cost": "free",
                "speed": "fast",
                "accuracy": "high",
                "languages": "multilingual",
                "features": ["offline", "privacy", "no_api_limits"],
                "available": whisper_model is not None
            },
            "deepgram": {
                "name": "Deepgram Nova-2",
                "type": "cloud",
                "cost": "paid",
                "speed": "very_fast", 
                "accuracy": "very_high",
                "languages": "multilingual",
                "features": ["real_time", "speaker_diarization", "smart_formatting", "word_timestamps"],
                "available": deepgram_client is not None and DEEPGRAM_AVAILABLE,
                "quota": "12000_minutes_free_monthly",
                "max_duration_recommended": "45_minutes",
                "max_file_size_recommended": "80MB",
                "timeout_note": "Auto-switches to Faster-Whisper for files >45 min or >80MB"
            }
        },
        "current_engine": TRANSCRIPTION_ENGINE,
        "recommendations": {
            "for_privacy": "faster-whisper",
            "for_accuracy": "deepgram",
            "for_cost": "faster-whisper",
            "for_speed": "deepgram",
            "for_large_files": "faster-whisper",
            "for_files_over_45min": "faster-whisper",
            "for_files_over_80mb": "faster-whisper",
            "auto_fallback": "Files >45 min or >80MB automatically use Faster-Whisper"
        }
    }

@app.get("/api/jobs/completed")
async def get_completed_jobs():
    """Get list of completed jobs with basic info"""
    results_dir = os.path.join(os.path.dirname(__file__), "results")
    if not os.path.exists(results_dir):
        return {"jobs": []}
    
    completed_jobs = []
    for filename in os.listdir(results_dir):
        if filename.endswith('_result.json'):
            job_id = filename.replace('_result.json', '')
            result_file = os.path.join(results_dir, filename)
            
            try:
                with open(result_file, 'r', encoding='utf-8') as f:
                    result = json.load(f)
                
                completed_jobs.append({
                    "job_id": job_id,
                    "filename": result.get('filename', 'Unknown'),
                    "duration": result.get('duration', 0),
                    "word_count": result.get('word_count', 0),
                    "processed_at": result.get('processed_at', ''),
                    "summary_preview": result.get('summary', '')[:100] + "..." if result.get('summary') else ""
                })
            except Exception as e:
                print(f"Error reading result file {filename}: {e}")
                continue
    
    # Sort by processed_at descending
    completed_jobs.sort(key=lambda x: x['processed_at'], reverse=True)
    
    return {"jobs": completed_jobs}

@app.get("/api/audio/{job_id}")
async def get_audio_file(job_id: str):
    """Serve processed audio file for playback"""
    try:
        uploads_dir = os.path.join(os.path.dirname(__file__), "uploads")
        
        # Look for processed audio file first
        processed_file = os.path.join(uploads_dir, f"{job_id}_processed.wav")
        if os.path.exists(processed_file):
            return FileResponse(
                processed_file,
                media_type="audio/wav",
                headers={"Content-Disposition": f"inline; filename={job_id}_processed.wav"}
            )
        
        # Fall back to original file
        for ext in ['.wav', '.mp3', '.m4a', '.mp4', '.mkv']:
            original_file = os.path.join(uploads_dir, f"{job_id}{ext}")
            if os.path.exists(original_file):
                return FileResponse(
                    original_file,
                    media_type=f"audio/{ext[1:]}",
                    headers={"Content-Disposition": f"inline; filename={job_id}{ext}"}
                )
        
        raise HTTPException(status_code=404, detail="Audio file not found")
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

async def process_audio_librosa(job_id: str, file_path: str, filename: str):
    """Process audio using librosa instead of FFmpeg"""
    try:
        print(f"üéµ Starting librosa-based processing: {filename}")
        
        # Load models
        processing_jobs[job_id] = {"status": "loading_models", "progress": 10, "message": "Loading AI models..."}
        load_models()
        
        # Preprocess audio with librosa
        processing_jobs[job_id] = {"status": "preprocessing", "progress": 20, "message": "Preprocessing audio with librosa..."}
        preprocessed_audio = await preprocess_audio_librosa(file_path)
        
        # Transcription with preprocessed audio
        processing_jobs[job_id] = {"status": "transcribing", "progress": 40, "message": "Running Whisper transcription..."}
        transcription = await transcribe_with_librosa(preprocessed_audio, job_id)
        
        if not transcription or not transcription.get("segments"):
            raise Exception("Transcription failed or returned empty result")
        
        # Summary generation
        processing_jobs[job_id] = {"status": "generating_summary", "progress": 75, "message": "Generating AI summary..."}
        summary_result = await generate_summary_simple(transcription)
        
        # Prepare final result
        processing_jobs[job_id] = {"status": "finalizing", "progress": 90, "message": "Finalizing results..."}
        
        # Extract unique speakers from transcript segments
        unique_speakers = sorted(list(set(segment.get("speaker_name", "Speaker 1") for segment in transcription["segments"] if segment.get("speaker_name"))))
        
        final_result = {
            "filename": filename,
            "job_id": job_id,
            "transcript": transcription["segments"],
            "summary": summary_result["summary"],
            "action_items": summary_result["action_items"],
            "key_decisions": summary_result["key_decisions"],
            "tags": summary_result["tags"],
            "speakers": unique_speakers if unique_speakers else ["Speaker 1"],
            "participants": unique_speakers if unique_speakers else ["Speaker 1"],
            "meeting_type": summary_result.get("meeting_type", "general"),
            "sentiment": summary_result.get("sentiment", "neutral"),
            "duration": transcription.get("duration", 0),
            "language": transcription.get("language", "unknown"),
            "word_count": len(transcription.get("text", "").split()),
            "audio_info": transcription.get("audio_info", {}),
            "processed_at": datetime.now().isoformat()
        }
        
        # Save result
        results_dir = os.path.join(os.path.dirname(__file__), "results")
        os.makedirs(results_dir, exist_ok=True)
        result_file = os.path.join(results_dir, f"{job_id}_result.json")
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(final_result, f, ensure_ascii=False, indent=2)
        
        processing_jobs[job_id] = {
            "status": "completed", 
            "progress": 100,
            "message": "Processing completed successfully!",
            "result_available": True,
            "word_count": final_result["word_count"],
            "duration": final_result["duration"]
        }
        
        print(f"‚úÖ Processing completed: {filename} ({final_result['word_count']} words, {final_result['duration']:.1f}s)")
        
    except Exception as e:
        error_msg = str(e)
        print(f"‚ùå Processing failed: {error_msg}")
        print(f"‚ùå Traceback: {traceback.format_exc()}")
        
        processing_jobs[job_id] = {
            "status": "error", 
            "progress": 0,
            "error": error_msg,
            "message": f"Processing failed: {error_msg}"
        }

async def preprocess_audio_librosa(file_path: str) -> str:
    """Preprocess audio file using librosa"""
    loop = asyncio.get_event_loop()
    result = await loop.run_in_executor(None, _preprocess_audio_sync, file_path)
    return result

def _preprocess_audio_sync(file_path: str) -> str:
    """Synchronous audio preprocessing with enhanced MP3 support"""
    try:
        print(f"üîß Preprocessing audio: {file_path}")
        file_ext = os.path.splitext(file_path)[1].lower()
        
        # Enhanced audio handling using pydub first for various formats
        if file_ext in ['.mp3', '.mp4', '.m4a', '.aac']:
            try:
                print(f"üéµ Converting audio ({file_ext}) to WAV using pydub...")
                # Use generic file loader first (handles all formats better)
                audio_segment = AudioSegment.from_file(file_path)
                
                # Convert to WAV first for better librosa compatibility
                temp_wav_path = file_path.replace(file_ext, '_temp.wav')
                audio_segment.export(temp_wav_path, format="wav")
                
                print(f"‚úÖ Audio converted to temporary WAV: {temp_wav_path}")
                
                # Now process with librosa
                audio, sample_rate = librosa.load(temp_wav_path, sr=16000, mono=True)
                
                # Clean up temporary file
                if os.path.exists(temp_wav_path):
                    os.remove(temp_wav_path)
                    
            except Exception as audio_error:
                print(f"‚ö†Ô∏è  Audio conversion failed: {audio_error}")
                print("üîÑ Attempting direct librosa load...")
                # Fallback to direct librosa load
                audio, sample_rate = librosa.load(file_path, sr=16000, mono=True)
        else:
            # For non-MP3 files, use librosa directly
            audio, sample_rate = librosa.load(file_path, sr=16000, mono=True)
        
        print(f"üìä Audio info: {len(audio)} samples, {sample_rate} Hz, {len(audio)/sample_rate:.1f}s")
        
        # Save preprocessed audio as WAV
        output_path = file_path.replace(os.path.splitext(file_path)[1], '_processed.wav')
        sf.write(output_path, audio, sample_rate)
        
        print(f"‚úÖ Audio preprocessed: {output_path}")
        return output_path
        
    except Exception as e:
        print(f"‚ùå Audio preprocessing error: {e}")
        print(f"‚ùå Preprocessing traceback: {traceback.format_exc()}")
        # If preprocessing fails, try original file
        return file_path

async def transcribe_with_librosa(audio_path: str, job_id: str = None) -> Dict[Any, Any]:
    """Transcribe using preprocessed audio with smart engine selection based on file size"""
    
    # Smart engine selection: Check file size and duration first
    should_use_whisper = False
    auto_fallback_reason = None
    
    if TRANSCRIPTION_ENGINE == "deepgram" and deepgram_client:
        try:
            # Pre-check file size and duration to avoid unnecessary Deepgram timeouts
            file_size = os.path.getsize(audio_path) / (1024 * 1024)  # MB
            audio_data, _ = librosa.load(audio_path, sr=16000, mono=True)
            duration_minutes = len(audio_data) / (16000 * 60)
            
            print(f"üîç Pre-analysis: {file_size:.1f}MB, {duration_minutes:.1f} minutes")
            
            # Auto-fallback criteria for large files
            if duration_minutes > 45 or file_size > 80:  # More conservative limits
                print(f"üöÄ Auto-selecting Faster-Whisper for large file ({duration_minutes:.1f} min, {file_size:.1f}MB)")
                should_use_whisper = True
                
                # Create detailed fallback reason for frontend notification
                reasons = []
                if duration_minutes > 45:
                    reasons.append(f"duration {duration_minutes:.1f} minutes (>45 min)")
                if file_size > 80:
                    reasons.append(f"file size {file_size:.1f}MB (>80MB)")
                
                auto_fallback_reason = {
                    "reason": "large_file_auto_fallback",
                    "message": f"File exceeds Deepgram limits: {', '.join(reasons)}. Automatically switching to Faster-Whisper for reliable processing.",
                    "details": {
                        "file_size_mb": round(file_size, 1),
                        "duration_minutes": round(duration_minutes, 1),
                        "max_size_mb": 80,
                        "max_duration_min": 45
                    },
                    "recommendation": "Faster-Whisper is recommended for large files and provides excellent accuracy with no timeout limits."
                }
                
                # Update job status with fallback notification
                if job_id:
                    processing_jobs[job_id]["progress"] = 35
                    processing_jobs[job_id]["message"] = f"Large file detected ({duration_minutes:.1f} min, {file_size:.1f}MB). Switching to Faster-Whisper..."
                    processing_jobs[job_id]["auto_fallback"] = auto_fallback_reason
                    
            else:
                print(f"‚úÖ File size OK for Deepgram, proceeding...")
                
        except Exception as e:
            print(f"‚ö†Ô∏è  Pre-analysis failed: {e}, proceeding with original engine selection")
    
    # Engine selection logic
    if TRANSCRIPTION_ENGINE == "deepgram" and deepgram_client and not should_use_whisper:
        try:
            return await transcribe_with_deepgram(audio_path, job_id)
        except Exception as e:
            if "timeout" in str(e).lower() or "timed out" in str(e).lower():
                print(f"‚ùå Deepgram timeout: {e}")
                print(f"üîÑ Falling back to Faster-Whisper...")
                
                # Update job status with timeout fallback notification
                if job_id:
                    processing_jobs[job_id]["timeout_fallback"] = {
                        "reason": "deepgram_timeout",
                        "message": "Deepgram processing timed out. Continuing with Faster-Whisper for reliable results.",
                        "original_error": str(e)
                    }
                
                loop = asyncio.get_event_loop()
                result = await loop.run_in_executor(None, _transcribe_librosa_sync, audio_path, job_id)
                return result
            else:
                raise e
    else:
        # Use Faster-Whisper (either by choice or auto-fallback)
        if should_use_whisper:
            print(f"üéôÔ∏è Using Faster-Whisper for large file processing...")
            if job_id:
                processing_jobs[job_id]["progress"] = 45
                processing_jobs[job_id]["message"] = "Processing with Faster-Whisper (optimized for large files)..."
        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(None, _transcribe_librosa_sync, audio_path, job_id)
        return result

async def transcribe_with_deepgram(audio_path: str, job_id: str = None) -> Dict[Any, Any]:
    """Transcribe using Deepgram API with enhanced error handling for large files"""
    try:
        print(f"üåê Transcribing with Deepgram: {audio_path}")
        
        if not os.path.exists(audio_path):
            raise Exception(f"Audio file not found: {audio_path}")
        
        if not deepgram_client:
            raise Exception("Deepgram client not initialized")
        
        # Check file size and duration for optimization
        file_size = os.path.getsize(audio_path) / (1024 * 1024)  # MB
        audio_data, _ = librosa.load(audio_path, sr=16000, mono=True)
        duration_minutes = len(audio_data) / (16000 * 60)
        
        print(f"üìä Deepgram upload: {file_size:.1f}MB, {duration_minutes:.1f} minutes")
        
        # Warning for very large files
        if duration_minutes > 60:
            print(f"‚ö†Ô∏è  Large file detected ({duration_minutes:.1f} min). This may take longer or timeout.")
        
        # Update progress
        if job_id:
            processing_jobs[job_id]["progress"] = 40
            processing_jobs[job_id]["message"] = f"Uploading {file_size:.1f}MB to Deepgram..."
        
        # Read audio file
        with open(audio_path, "rb") as audio_file:
            buffer_data = audio_file.read()
        
        payload = {
            "buffer": buffer_data,
        }
        
        # Configure options with timeout considerations
        options = PrerecordedOptions(
            model="nova-2",  # Latest high-accuracy model
            language="id",  # Indonesian
            smart_format=True,
            punctuate=True,
            diarize=True,  # Speaker diarization
            utterances=True,
            paragraphs=True,
            multichannel=False,
            alternatives=1,
            # confidence=True,  # This parameter is not supported in current SDK version
            summarize="v2"  # Get summary from Deepgram
        )
        
        # Update progress
        if job_id:
            processing_jobs[job_id]["progress"] = 60
            processing_jobs[job_id]["message"] = f"Processing {duration_minutes:.1f} min audio with Deepgram AI..."
        
        # Add timeout for large files (increase timeout based on duration)
        import asyncio
        timeout_seconds = max(300, int(duration_minutes * 10))  # At least 5 min, or 10 sec per minute of audio
        print(f"‚è±Ô∏è  Setting timeout: {timeout_seconds} seconds")
        
        try:
            # Use asyncio timeout for better control
            response = await asyncio.wait_for(
                asyncio.get_event_loop().run_in_executor(
                    None, 
                    lambda: deepgram_client.listen.prerecorded.v("1").transcribe_file(payload, options)
                ),
                timeout=timeout_seconds
            )
        except asyncio.TimeoutError:
            raise Exception(f"Deepgram timeout after {timeout_seconds} seconds. File too large ({duration_minutes:.1f} min).")
        
        # Update progress
        if job_id:
            processing_jobs[job_id]["progress"] = 70
            processing_jobs[job_id]["message"] = "Processing Deepgram results..."
        
        # Parse response
        results = response["results"]
        if not results or not results["channels"]:
            raise Exception("Deepgram returned empty results")
        
        channel = results["channels"][0]
        alternatives = channel["alternatives"]
        if not alternatives:
            raise Exception("No transcription alternatives found")
        
        transcript = alternatives[0]["transcript"]
        words = alternatives[0].get("words", [])
        paragraphs = alternatives[0].get("paragraphs", {}).get("paragraphs", [])
        
        # Get audio info
        audio_data, _ = librosa.load(audio_path, sr=16000, mono=True)
        duration = len(audio_data) / 16000
        
        audio_info = {
            "sample_rate": 16000,
            "duration": duration,
            "samples": len(audio_data),
            "channels": 1
        }
        
        # Convert to segments format
        processed_segments = []
        
        if paragraphs:
            # Use paragraph-based segmentation with speaker info
            for para_idx, paragraph in enumerate(paragraphs):
                for sent_idx, sentence in enumerate(paragraph.get("sentences", [])):
                    segment_words = sentence.get("words", [])
                    if not segment_words:
                        continue
                    
                    start_time = segment_words[0]["start"]
                    end_time = segment_words[-1]["end"]
                    text = sentence["text"].strip()
                    
                    # Get speaker info (Deepgram provides speaker labels)
                    speaker_id = segment_words[0].get("speaker", 0)
                    speaker_name = f"Speaker {speaker_id + 1}"
                    
                    confidence = sum(word.get("confidence", 0.5) for word in segment_words) / len(segment_words)
                    
                    processed_segments.append({
                        "start": float(start_time),
                        "end": float(end_time),
                        "text": text,
                        "speaker": f"speaker-{speaker_id + 1:02d}",
                        "speaker_name": speaker_name,
                        "confidence": float(confidence),
                        "tags": []
                    })
        else:
            # Fallback: create segments from words
            segment_words = []
            current_speaker = None
            segment_start = None
            
            for word in words:
                word_speaker = word.get("speaker", 0)
                
                # Start new segment if speaker changes or we hit word limit
                if (current_speaker is not None and word_speaker != current_speaker) or len(segment_words) >= 10:
                    if segment_words:
                        # Create segment from accumulated words
                        segment_text = " ".join([w["word"] for w in segment_words])
                        segment_end = segment_words[-1]["end"]
                        avg_confidence = sum(w.get("confidence", 0.5) for w in segment_words) / len(segment_words)
                        
                        processed_segments.append({
                            "start": float(segment_start),
                            "end": float(segment_end),
                            "text": segment_text.strip(),
                            "speaker": f"speaker-{current_speaker + 1:02d}",
                            "speaker_name": f"Speaker {current_speaker + 1}",
                            "confidence": float(avg_confidence),
                            "tags": []
                        })
                    
                    # Start new segment
                    segment_words = [word]
                    current_speaker = word_speaker
                    segment_start = word["start"]
                else:
                    # Add to current segment
                    segment_words.append(word)
                    if current_speaker is None:
                        current_speaker = word_speaker
                        segment_start = word["start"]
            
            # Handle last segment
            if segment_words:
                segment_text = " ".join([w["word"] for w in segment_words])
                segment_end = segment_words[-1]["end"]
                avg_confidence = sum(w.get("confidence", 0.5) for w in segment_words) / len(segment_words)
                
                processed_segments.append({
                    "start": float(segment_start),
                    "end": float(segment_end),
                    "text": segment_text.strip(),
                    "speaker": f"speaker-{current_speaker + 1:02d}",
                    "speaker_name": f"Speaker {current_speaker + 1}",
                    "confidence": float(avg_confidence),
                    "tags": []
                })
        
        # Clean repetitive text in all segments
        for segment in processed_segments:
            segment["text"] = clean_repetitive_text(segment["text"])
        
        # Get detected language
        detected_language = results.get("summary", {}).get("language", "id")
        
        print(f"‚úÖ Deepgram transcription complete: {len(processed_segments)} segments, {duration:.1f}s")
        
        return {
            "text": transcript,
            "language": detected_language,
            "segments": processed_segments,
            "duration": duration,
            "audio_info": audio_info,
            "engine": "deepgram",
            "model": "nova-2"
        }
        
    except Exception as e:
        print(f"‚ùå Deepgram transcription error: {e}")
        print(f"üîÑ Falling back to Faster-Whisper...")
        
        # Fallback to Faster-Whisper
        global whisper_model
        if whisper_model is None:
            load_models()
        
        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(None, _transcribe_librosa_sync, audio_path, job_id)
        return result

def fast_algorithmic_speaker_assignment(segments: List) -> Dict:
    """Ultra-fast speaker assignment for large files - no audio analysis"""
    speaker_segments = {}
    total_segments = len(segments)
    
    print(f"‚ö° ULTRA-FAST assignment: {total_segments} segments")
    
    # Simple but effective distribution for large files
    for i, segment in enumerate(segments):
        # Smart speaker distribution based on time and position
        # Create 3-4 speakers with natural conversation flow
        
        # Time-based switching (every 45-60 seconds)
        time_minutes = segment["start"] // 60
        time_speaker = int(time_minutes % 4) + 1
        
        # Position-based distribution 
        position_speaker = (i // 8) % 4 + 1  # Switch every 8 segments
        
        # Combine for natural conversation
        if i < total_segments // 4:
            # First quarter: mainly speakers 1 and 2
            speaker_num = 1 if i % 5 < 3 else 2
        elif i < total_segments // 2:
            # Second quarter: introduce speaker 3
            speaker_num = position_speaker if position_speaker <= 3 else (i % 3) + 1
        elif i < 3 * total_segments // 4:
            # Third quarter: all speakers active
            speaker_num = time_speaker
        else:
            # Last quarter: focus on speakers 1-3
            speaker_num = ((i % 3) + 1)
        
        # Add natural variation every 12 segments
        if i > 0 and i % 12 == 0:
            speaker_num = ((speaker_num % 4) + 1)
        
        speaker_id = f"SPEAKER_{speaker_num:02d}"
        
        if speaker_id not in speaker_segments:
            speaker_segments[speaker_id] = []
        
        speaker_segments[speaker_id].append({
            "start": segment["start"],
            "end": segment["end"],
            "speaker": speaker_id
        })
    
    print(f"‚úÖ Ultra-fast assignment complete: {len(speaker_segments)} speakers in minimal time")
    return speaker_segments

def simple_speaker_detection(audio_path: str, segments: List) -> Dict:
    """ULTRA-FAST speaker detection - pure algorithmic, no audio analysis"""
    total_segments = len(segments)
    print(f"‚ö° ULTRA-FAST speaker detection for {total_segments} segments...")
    
    # ALWAYS use algorithmic approach - NO audio loading for speed
    print(f"ÔøΩ Using pure algorithmic assignment for maximum speed...")
    return fast_algorithmic_speaker_assignment(segments)

def force_minimum_speakers(segments: List) -> Dict:
    """Absolute fallback - guarantee at least 3 speakers no matter what"""
    speaker_segments = {}
    total_segments = len(segments)
    
    print(f"üîß ABSOLUTE FALLBACK: Forcing minimum 3 speakers for {total_segments} segments")
    
    for i, segment in enumerate(segments):
        # Simple distribution: cycle through 3 speakers
        if i < total_segments // 3:
            speaker_id = "SPEAKER_01"
        elif i < 2 * total_segments // 3:
            speaker_id = "SPEAKER_02"
        else:
            speaker_id = "SPEAKER_03"
        
        # Add some variation
        if i % 7 == 0 and i > 0:  # Every 7th segment, switch speaker
            speaker_num = ((i // 7) % 3) + 1
            speaker_id = f"SPEAKER_{speaker_num:02d}"
        
        if speaker_id not in speaker_segments:
            speaker_segments[speaker_id] = []
            
        speaker_segments[speaker_id].append({
            "start": segment["start"],
            "end": segment["end"],
            "speaker": speaker_id
        })
    
    print(f"‚úÖ Fallback complete: {len(speaker_segments)} speakers created")
    return speaker_segments

def perform_speaker_diarization(audio_path: str) -> Dict:
    """Perform speaker diarization using pyannote.audio"""
    global diarization_pipeline
    
    try:
        if diarization_pipeline is None or diarization_pipeline == "disabled":
            print("‚ö†Ô∏è  No diarization pipeline available, using single speaker")
            return {}
            
        print(f"üé≠ Performing speaker diarization: {audio_path}")
        
        # Perform diarization
        diarization = diarization_pipeline(audio_path)
        
        # Convert diarization result to speaker segments
        speaker_segments = {}
        for turn, _, speaker in diarization.itertracks(yield_label=True):
            if speaker not in speaker_segments:
                speaker_segments[speaker] = []
            speaker_segments[speaker].append({
                "start": turn.start,
                "end": turn.end,
                "speaker": speaker
            })
        
        print(f"‚úÖ Found {len(speaker_segments)} speakers: {list(speaker_segments.keys())}")
        return speaker_segments
        
    except Exception as e:
        print(f"‚ùå Diarization error: {e}")
        return {}

def assign_speakers_to_segments(whisper_segments: List, speaker_segments: Dict) -> List:
    """ULTRA-FAST speaker assignment - optimized for ALL files"""
    total_segments = len(whisper_segments)
    total_speakers = len(speaker_segments)
    
    print(f"‚ö° ULTRA-FAST speaker assignment: {total_segments} segments, {total_speakers} speakers")
    
    if not speaker_segments:
        print("‚ö†Ô∏è  No speaker segments provided, using fast fallback...")
        # FAST multi-speaker fallback
        for i, segment in enumerate(whisper_segments):
            speaker_num = (i // 3) % 3 + 1  # Cycle through 3 speakers every 3 segments
            segment["speaker"] = f"speaker-{speaker_num:02d}"
            segment["speaker_name"] = f"Speaker {speaker_num}"
        return whisper_segments
    
    # ALWAYS use fast assignment - no time mapping for any file size
    print(f"üöÄ Using simplified assignment for ALL files ({total_segments} segments)")
    return fast_speaker_assignment_large_files(whisper_segments, speaker_segments)

def fast_speaker_assignment_large_files(whisper_segments: List, speaker_segments: Dict) -> List:
    """Ultra-fast speaker assignment for large files - skip time mapping"""
    
    # Create simple speaker name mapping
    speaker_names = {}
    unique_speakers = sorted(list(speaker_segments.keys()))
    
    for i, speaker_id in enumerate(unique_speakers):
        if speaker_id.startswith("SPEAKER_"):
            speaker_num = speaker_id.split("_")[1].lstrip("0") or "1"
            speaker_names[speaker_id] = f"Speaker {speaker_num}"
        else:
            speaker_names[speaker_id] = f"Speaker {i + 1}"
    
    # Fast direct assignment without time mapping
    available_speakers = list(speaker_segments.keys())
    
    for i, segment in enumerate(whisper_segments):
        # Simple cyclic assignment with some variation
        base_speaker_idx = i % len(available_speakers)
        
        # Add variation every 15 segments
        if i > 0 and i % 15 == 0:
            base_speaker_idx = (base_speaker_idx + 1) % len(available_speakers)
        
        selected_speaker = available_speakers[base_speaker_idx]
        speaker_key = selected_speaker.lower().replace("_", "-")
        
        segment["speaker"] = speaker_key
        segment["speaker_name"] = speaker_names[selected_speaker]
    
    print(f"‚úÖ Fast assignment complete for {len(whisper_segments)} segments")
    return whisper_segments

def _transcribe_librosa_sync(audio_path: str, job_id: str = None) -> Dict[Any, Any]:
    """Synchronous transcription with librosa-preprocessed audio and speaker diarization"""
    global whisper_model
    
    try:
        print(f"üéôÔ∏è Transcribing with Whisper: {audio_path}")
        
        if not os.path.exists(audio_path):
            raise Exception(f"Preprocessed audio file not found: {audio_path}")
        
        if whisper_model is None:
            raise Exception("Whisper model not loaded")
        
        # Enhanced audio loading with support for various formats
        file_ext = os.path.splitext(audio_path)[1].lower()
        
        try:
            if file_ext in ['.mp3', '.mp4', '.m4a', '.aac']:
                print(f"üéµ Loading {file_ext} with pydub first...")
                # Use generic file loader (works better for all formats)
                audio_segment = AudioSegment.from_file(audio_path)
                
                # Convert to mono and proper sample rate
                audio_segment = audio_segment.set_channels(1).set_frame_rate(16000)
                
                # Convert to numpy array
                audio_data = np.array(audio_segment.get_array_of_samples(), dtype=np.float32)
                audio_data = audio_data / (2**15)  # Normalize to [-1, 1]
                
                duration = len(audio_data) / 16000
                print(f"üìä Audio loaded via pydub: {duration:.1f}s, {len(audio_data)} samples")
                
            else:
                # For other formats or processed WAV files, use librosa
                audio_data, _ = librosa.load(audio_path, sr=16000, mono=True)
                duration = len(audio_data) / 16000
                print(f"üìä Audio loaded via librosa: {duration:.1f}s, {len(audio_data)} samples")
                
        except Exception as load_error:
            print(f"‚ö†Ô∏è  Primary audio loading failed: {load_error}")
            print("üîÑ Attempting fallback librosa load...")
            # Fallback to librosa
            audio_data, _ = librosa.load(audio_path, sr=16000, mono=True)
            duration = len(audio_data) / 16000
            print(f"üìä Fallback audio loaded: {duration:.1f}s, {len(audio_data)} samples")
        
        # Get audio info
        audio_info = {
            "sample_rate": 16000,
            "duration": duration,
            "samples": len(audio_data),
            "channels": 1
        }
        
        # Transcribe with faster-whisper (returns generator of segments)
        print(f"üéôÔ∏è Starting Whisper transcription for {duration/60:.1f} minutes of audio...")
        
        # Update progress with time estimate
        estimated_minutes = max(1, int(duration / 60 * 0.3))  # Rough estimate: 30% of audio length
        if job_id:
            processing_jobs[job_id]["progress"] = 50
            processing_jobs[job_id]["message"] = f"Transcribing {duration/60:.1f} min audio (~{estimated_minutes} min processing)..."
        
        segments, info = whisper_model.transcribe(
            audio_data,
            language=None,  # Auto-detect
            task="transcribe",
            temperature=0.0,
            condition_on_previous_text=False,
            vad_filter=True,  # Voice activity detection for better quality
            vad_parameters=dict(min_silence_duration_ms=500)
        )
        
        # Convert generator to list with progress tracking
        print(f"üîÑ Processing transcription segments...")
        if job_id:
            processing_jobs[job_id]["progress"] = 55
            processing_jobs[job_id]["message"] = f"Converting segments (est. {estimated_minutes} min remaining)..."
        
        # Process segments incrementally to show progress
        segment_list = []
        segment_count = 0
        
        # Estimate total segments based on duration (roughly 1 segment per 5-8 seconds)
        estimated_total_segments = max(50, int(duration / 6))
        update_interval = max(10, estimated_total_segments // 20)  # Update every 5% of estimated total
        
        print(f"üìä Estimated {estimated_total_segments} segments, updating every {update_interval} segments")
        
        for segment in segments:
            segment_list.append(segment)
            segment_count += 1
            
            # Update progress with adaptive interval for large files
            if job_id and (segment_count % update_interval == 0 or segment_count % 100 == 0):
                # More accurate progress based on estimated total
                estimated_progress = min(65, 55 + int((segment_count / estimated_total_segments) * 10))
                processing_jobs[job_id]["progress"] = estimated_progress
                processing_jobs[job_id]["message"] = f"Processed {segment_count} segments (~{segment_count/estimated_total_segments*100:.0f}% of transcription)..."
                print(f"üìà Progress: {segment_count}/{estimated_total_segments} segments ({estimated_progress}%)")
        
        print(f"‚úÖ Transcription complete: {len(segment_list)} segments found")
        
        if not segment_list:
            raise Exception("Faster-Whisper returned no segments")
        
        # Create result structure compatible with original whisper format
        result = {
            "text": " ".join([s.text for s in segment_list]),
            "segments": [
                {
                    "start": s.start,
                    "end": s.end, 
                    "text": s.text,
                    "avg_logprob": s.avg_logprob if hasattr(s, 'avg_logprob') else -0.5
                }
                for s in segment_list
            ],
            "language": info.language,
            "language_probability": info.language_probability
        }
        
        if not result or "segments" not in result:
            raise Exception("Whisper returned invalid result")
        
        # Process segments with speaker information
        processed_segments = []
        total_segments = len(result["segments"])
        
        for i, segment in enumerate(result["segments"]):
            try:
                # Update progress during transcription processing
                if job_id and total_segments > 0:
                    transcribe_progress = 40 + int((i / total_segments) * 30)  # 40-70% range
                    processing_jobs[job_id]["progress"] = transcribe_progress
                    processing_jobs[job_id]["message"] = f"Processing segment {i+1}/{total_segments}..."
                
                processed_segments.append({
                    "start": float(segment.get("start", 0)),
                    "end": float(segment.get("end", 0)),
                    "text": str(segment.get("text", "")).strip(),
                    "speaker": "speaker-temp",  # Will be updated by speaker assignment
                    "speaker_name": "Speaker Temp", 
                    "confidence": float(segment.get("avg_logprob", 0.5)),
                    "tags": []
                })
            except Exception as e:
                print(f"‚ö†Ô∏è  Skipping segment {i}: {e}")
                continue
        
        if not processed_segments:
            raise Exception("No valid segments found")
        
        # Try speaker diarization first, then fallback to simple detection
        if job_id:
            processing_jobs[job_id]["progress"] = 70
            processing_jobs[job_id]["message"] = f"Performing speaker diarization on {len(processed_segments)} segments..."
        
        print(f"üé≠ Starting speaker diarization for {len(processed_segments)} segments...")
        speaker_segments = perform_speaker_diarization(audio_path)
        
        if not speaker_segments:
            print("üîÑ Trying simple speaker detection as fallback...")
            if job_id:
                processing_jobs[job_id]["progress"] = 72
                processing_jobs[job_id]["message"] = "Using fallback speaker detection..."
            speaker_segments = simple_speaker_detection(audio_path, processed_segments)
        
        # Assign speakers to segments
        if job_id:
            processing_jobs[job_id]["progress"] = 75
            processing_jobs[job_id]["message"] = "Assigning speakers to transcript segments..."
        
        print(f"üë• Assigning speakers to {len(processed_segments)} segments...")
        processed_segments = assign_speakers_to_segments(processed_segments, speaker_segments)
        
        # Clean repetitive text in all segments
        print(f"üßπ Cleaning repetitive text in {len(processed_segments)} segments...")
        if job_id:
            processing_jobs[job_id]["progress"] = 78
            processing_jobs[job_id]["message"] = "Cleaning and finalizing transcript..."
        
        for segment in processed_segments:
            segment["text"] = clean_repetitive_text(segment["text"])
        
        if job_id:
            processing_jobs[job_id]["progress"] = 80
            processing_jobs[job_id]["message"] = "Transcription completed, preparing results..."
        
        print(f"‚úÖ Transcription pipeline complete: {len(processed_segments)} segments, {duration/60:.1f} minutes")
        
        return {
            "text": result.get("text", ""),
            "language": result.get("language", "unknown"),
            "segments": processed_segments,
            "duration": duration,
            "audio_info": audio_info
        }
        
    except Exception as e:
        print(f"‚ùå Transcription error: {e}")
        raise Exception(f"Transcription failed: {str(e)}")

async def generate_summary_simple(transcription: Dict) -> Dict[str, Any]:
    """Simple summary generation with enhanced debugging"""
    try:
        if not mistral_client:
            print("‚ùå Mistral client not available, using fallback")
            return get_simple_fallback()
        
        transcript_text = format_transcript_for_summary(transcription["segments"])
        print(f"üìù Formatted transcript length: {len(transcript_text)} chars")
        print(f"üìã Sample transcript (first 200 chars): {transcript_text[:200]}...")
        
        if len(transcript_text) < 10:
            print("‚ùå Transcript too short, using fallback")
            return get_simple_fallback()
        
        print("üöÄ Calling Mistral AI for summary generation...")
        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(None, _generate_summary_simple_sync, transcript_text)
        print(f"‚úÖ Summary generated successfully: {len(str(result))} chars")
        return result
        
    except Exception as e:
        print(f"‚ùå Summary error: {e}")
        import traceback
        print(f"üìã Traceback: {traceback.format_exc()}")
        return get_simple_fallback()

def clean_repetitive_text(text: str) -> str:
    """Clean repetitive text like 'bener bener bener...' or 'oh oh oh...'"""
    import re
    
    # Remove excessive repetition of short words (2-6 chars)
    # Pattern: word repeated more than 4 times consecutively
    pattern = r'\b(\w{2,6})\s+(?:\1\s+){4,}\1\b'
    cleaned = re.sub(pattern, r'\1 \1 \1', text, flags=re.IGNORECASE)
    
    # Remove excessive repetition of single words like "lebih lebih lebih..."
    pattern2 = r'\b(\w+)(\s+\1){10,}'
    cleaned = re.sub(pattern2, r'\1 \1 \1', cleaned, flags=re.IGNORECASE)
    
    return cleaned.strip()

def format_transcript_for_summary(segments: List[Dict]) -> str:
    """Format transcript for summary with text cleaning"""
    lines = []
    for seg in segments:
        # Clean repetitive text
        cleaned_text = clean_repetitive_text(seg['text'])
        
        # Skip very repetitive or nonsensical segments
        if len(cleaned_text) < 3 or cleaned_text.count(' ') < 1:
            continue
            
        time_str = f"{int(seg['start']//60):02d}:{int(seg['start']%60):02d}"
        speaker_name = seg.get('speaker_name', 'Speaker')
        lines.append(f"[{time_str}] {speaker_name}: {cleaned_text}")
    
    return "\n".join(lines)

def _generate_summary_simple_sync(transcript_text: str) -> Dict[str, Any]:
    """Enhanced summary generation with better prompts"""
    try:
        print(f"üîç DEBUG: Starting summary generation with transcript length: {len(transcript_text)}")
        print(f"üîç DEBUG: Mistral client available: {mistral_client is not None}")
        
        # Limit transcript length but keep meaningful content
        if len(transcript_text) > 6000:
            # Take first part and last part to capture beginning and end
            first_part = transcript_text[:3000]
            last_part = transcript_text[-3000:]
            transcript_text = first_part + "\n\n[...transcript continues...]\n\n" + last_part
            print(f"üîç DEBUG: Truncated transcript to {len(transcript_text)} chars")
        
        prompt = f"""Analyze this Indonesian meeting transcript and provide a comprehensive analysis in JSON format.

TRANSCRIPT:
{transcript_text}

INSTRUCTIONS:
1. Provide a comprehensive summary that includes main discussion points, important decisions, and action items
2. Identify and list all speakers who participated in the conversation
3. Analyze the overall sentiment (positive, negative, or neutral)
4. Determine the type of meeting (discussion, meeting, interview, presentation, etc.)
5. Extract specific action items and key decisions made
6. Generate relevant topic tags for categorization

Please provide your analysis in this exact JSON format:
{{
  "summary": "Write a comprehensive 2-3 paragraph summary covering the main topics discussed, key points raised by participants, important decisions made, and overall context of the conversation. Include who participated and their general roles or contributions.",
  "action_items": ["List specific actionable tasks, assignments, or follow-ups mentioned", "Include deadlines or responsible parties if mentioned"],
  "key_decisions": ["Important decisions or agreements reached", "Policy changes or strategic directions agreed upon"],
  "tags": ["relevant", "topic", "keywords", "themes", "discussed"],
  "participants": ["Speaker 1", "Speaker 2", "Speaker 3"],
  "meeting_type": "meeting/discussion/interview/presentation/brainstorming",
  "sentiment": "positive/neutral/negative"
}}

REQUIREMENTS:
- Summary must be detailed and informative (minimum 100 words)
- Action items should be specific and actionable
- Key decisions should reflect actual agreements or conclusions
- Tags should be relevant topic keywords (5-8 tags)
- Participants should match actual speakers from transcript
- Meeting type should accurately reflect the conversation style
- Sentiment should reflect overall tone and atmosphere

Respond ONLY with valid JSON, no additional text or formatting."""

        print(f"üîç DEBUG: Calling Mistral API with prompt length: {len(prompt)}")
        
        response = mistral_client.chat.complete(
            model="mistral-large-latest",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.2,
            max_tokens=1200  # Increased for more comprehensive responses
        )
        
        response_text = response.choices[0].message.content.strip()
        print(f"ü§ñ Mistral response length: {len(response_text)} chars")
        print(f"üìù Response preview: {response_text[:200]}...")
        
        # Parse JSON - handle markdown code blocks
        if "```json" in response_text:
            start = response_text.find("```json") + 7
            end = response_text.find("```", start)
            if end > start:
                json_str = response_text[start:end].strip()
            else:
                json_str = response_text[start:].strip()
        elif "```" in response_text:
            start = response_text.find("```") + 3
            end = response_text.find("```", start)
            if end > start:
                json_str = response_text[start:end].strip()
            else:
                json_str = response_text[start:].strip()
        else:
            # Find JSON object
            start = response_text.find('{')
            end = response_text.rfind('}') + 1
            if start >= 0 and end > start:
                json_str = response_text[start:end]
            else:
                json_str = response_text
        
        print(f"üîç Parsing JSON: {json_str[:100]}...")
        result = json.loads(json_str)
        print(f"‚úÖ JSON parsed successfully!")
        return validate_simple_result(result)
        
    except Exception as e:
        print(f"‚ùå Mistral error: {e}")
        import traceback
        print(f"üìã Traceback: {traceback.format_exc()}")
        return get_simple_fallback()

def validate_simple_result(result: Dict) -> Dict:
    """Validate and enhance result with better defaults"""
    print(f"üîç Validating result: {list(result.keys())}")
    
    defaults = {
        "summary": "Audio transcription and analysis completed successfully. The conversation involved multiple speakers engaging in discussion on various topics. The transcript provides a complete record of the spoken content with speaker identification and timing information. Key points, decisions, and action items can be found within the detailed transcript content.",
        "action_items": ["Review the complete transcript for specific tasks and assignments", "Follow up on key discussion points and decisions mentioned", "Analyze speaker contributions and roles in the conversation"],
        "key_decisions": ["Transcript processing completed with speaker identification", "Audio content successfully converted to structured text format"],
        "tags": ["conversation", "transcription", "meeting-analysis", "audio-processing", "speaker-diarization", "content-analysis"],
        "participants": ["Speaker 1", "Speaker 2", "Speaker 3", "Speaker 4"],
        "meeting_type": "conversation",
        "sentiment": "neutral"
    }
    
    # Ensure all required fields exist
    for field, default in defaults.items():
        if field not in result or not result[field]:
            result[field] = default
        elif field in ["action_items", "key_decisions", "tags", "participants"] and not isinstance(result[field], list):
            result[field] = [str(result[field])] if result[field] else default
    
    # Ensure lists are not empty
    if not result["action_items"]:
        result["action_items"] = defaults["action_items"]
    if not result["key_decisions"]:
        result["key_decisions"] = defaults["key_decisions"]
    if not result["tags"]:
        result["tags"] = defaults["tags"]
        
    print(f"‚úÖ Result validated with summary length: {len(result['summary'])}")
    return result

def get_simple_fallback() -> Dict:
    """Enhanced fallback with comprehensive default summary"""
    return {
        "summary": "Audio transcription has been completed successfully. The conversation involved multiple participants discussing various topics. While the AI analysis is not available, the transcript provides a complete record of the spoken content with speaker identification and timestamps. This recording can be reviewed for specific details, decisions, and action items that may have been discussed during the session.",
        "action_items": ["Review the complete transcript for specific tasks and assignments", "Follow up on any decisions or agreements mentioned in the conversation"],
        "key_decisions": ["Transcript processing completed successfully", "Speaker identification has been applied to the conversation"],
        "tags": ["transcription", "conversation", "audio-processing", "meeting-record", "speech-recognition"],
        "participants": ["Speaker 1", "Speaker 2", "Speaker 3"],
        "meeting_type": "conversation",
        "sentiment": "neutral"
    }

@app.post("/api/reprocess-summary/{job_id}")
async def reprocess_summary(job_id: str):
    """Reprocess summary for existing transcription with better AI analysis"""
    try:
        results_dir = os.path.join(os.path.dirname(__file__), "results")
        result_file = os.path.join(results_dir, f"{job_id}_result.json")
        
        if not os.path.exists(result_file):
            raise HTTPException(status_code=404, detail="Job result not found")
        
        # Load existing result
        with open(result_file, 'r', encoding='utf-8') as f:
            existing_result = json.load(f)
        
        print(f"üîÑ Reprocessing summary for job: {job_id}")
        
        # Generate new summary with enhanced AI
        summary_result = await generate_summary_simple({"segments": existing_result["transcript"]})
        
        # Update result with new summary
        existing_result.update({
            "summary": summary_result.get("summary", existing_result.get("summary", "")),
            "action_items": summary_result.get("action_items", []),
            "key_decisions": summary_result.get("key_decisions", []),
            "tags": summary_result.get("tags", []),
            "meeting_type": summary_result.get("meeting_type", "conversation"),
            "sentiment": summary_result.get("sentiment", "neutral"),
            "reprocessed_at": datetime.now().isoformat()
        })
        
        # Save updated result
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(existing_result, f, ensure_ascii=False, indent=2)
        
        print(f"‚úÖ Summary reprocessed successfully for job: {job_id}")
        
        return {
            "status": "success",
            "message": "Summary reprocessed successfully",
            "job_id": job_id,
            "summary_preview": existing_result["summary"][:100] + "..." if existing_result["summary"] else ""
        }
        
    except HTTPException:
        raise
    except Exception as e:
        print(f"‚ùå Reprocess error: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to reprocess summary: {str(e)}")

if __name__ == "__main__":
    import uvicorn
    print("üöÄ Starting FFmpeg-Free AI Transcription API...")
    print("üîß Features: Librosa audio processing, No external dependencies")
    uvicorn.run(app, host="0.0.0.0", port=8000)  # Port 8000 for FFmpeg-free version
