#!/usr/bin/env python3
"""
COMPATIBLE SIMPLE TRANSCRIPTION ENGINE
- Fast performance (simple logic)
- Same JSON response format as complex engine
- Full compatibility with existing frontend
- No breaking changes
"""

from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
import os
import asyncio
from datetime import datetime
import json
import time
from pathlib import Path
import subprocess
import shutil

# Import prompts system (keeping your sophisticated prompting)
from prompts import get_unified_analysis_prompt, get_fallback_responses, truncate_transcript

# Import API providers (keeping your AI integration)
try:
    from api_providers import initialize_providers, call_api
    API_PROVIDERS_AVAILABLE = True
    print("‚úÖ API providers available for advanced analysis")
except ImportError:
    API_PROVIDERS_AVAILABLE = False
    print("‚ö†Ô∏è  API providers not available - using simple fallback")

# Import FAISS offline chat system
try:
    from faiss_chat_system import FAISSChatSystem
    FAISS_CHAT_AVAILABLE = True
    print("‚úÖ FAISS offline chat system available")
except ImportError:
    FAISS_CHAT_AVAILABLE = False
    print("‚ö†Ô∏è FAISS chat system not available")

# Import Multi-Model Chat System that handles FAISS + Mistral smartly
try:
    from multi_model_chat import MultiModelChatSystem
    MULTI_MODEL_CHAT_AVAILABLE = True
    print("‚úÖ Multi-model chat system (FAISS + Mistral) available")
except ImportError:
    MULTI_MODEL_CHAT_AVAILABLE = False
    print("‚ö†Ô∏è Multi-model chat system not available")

# Simple whisper import - try both options
try:
    import whisper
    WHISPER_TYPE = "openai"
    print("‚úÖ Using OpenAI Whisper")
except ImportError:
    try:
        from faster_whisper import WhisperModel
        WHISPER_TYPE = "faster"
        print("‚úÖ Using Faster-Whisper")
    except ImportError:
        print("‚ùå No Whisper library available!")
        exit(1)

app = FastAPI(title="Fast AI Transcription - Compatible", version="2.0.0")

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Global model storage
whisper_model = None
processing_jobs = {}

# Initialize FAISS chat system
faiss_chat = None
if FAISS_CHAT_AVAILABLE:
    faiss_chat = FAISSChatSystem(data_dir="./results")
    print("ü§ñ FAISS offline chat system initialized")

# Initialize Multi-Model Chat System (preferred for both FAISS + Mistral)
multi_model_chat = None
if MULTI_MODEL_CHAT_AVAILABLE:
    multi_model_chat = MultiModelChatSystem(data_dir="./results")
    print("ü§ñ Multi-model chat system initialized (FAISS + Mistral)")

# Simple model options
MODEL_OPTIONS = {
    "tiny": {"size": "~39MB", "speed": "fastest", "accuracy": "basic"},
    "base": {"size": "~74MB", "speed": "fast", "accuracy": "good"}, 
    "small": {"size": "~244MB", "speed": "medium", "accuracy": "better"},
    "medium": {"size": "~769MB", "speed": "slow", "accuracy": "high"}
}

def convert_to_mp3(input_path, output_path):
    """Convert video/audio to MP3 using ffmpeg"""
    try:
        if not shutil.which("ffmpeg"):
            print("‚ö†Ô∏è  ffmpeg not found - using original file")
            return str(input_path)
        
        print(f"üîÑ Converting {input_path.name} to MP3...")
        start_time = time.time()
        
        cmd = [
            "ffmpeg", "-i", str(input_path), 
            "-vn", "-acodec", "mp3", "-ab", "128k",
            "-ar", "16000", "-ac", "1", "-y", str(output_path)
        ]
        
        result = subprocess.run(cmd, capture_output=True, text=True)
        
        if result.returncode == 0:
            convert_time = time.time() - start_time
            input_size = input_path.stat().st_size / (1024 * 1024)
            output_size = output_path.stat().st_size / (1024 * 1024)
            
            print(f"‚úÖ Conversion: {input_size:.1f}MB ‚Üí {output_size:.1f}MB in {convert_time:.2f}s")
            return str(output_path)
        else:
            print(f"‚ö†Ô∏è  Conversion failed, using original")
            return str(input_path)
            
    except Exception as e:
        print(f"‚ö†Ô∏è  Conversion error: {e}")
        return str(input_path)

def should_convert_to_mp3(filename):
    """Check if file should be converted to MP3"""
    video_extensions = ['.mp4', '.mov', '.avi', '.mkv', '.webm', '.flv']
    return Path(filename).suffix.lower() in video_extensions

def load_simple_model(model_name="tiny"):
    """Load simple whisper model"""
    global whisper_model
    
    if whisper_model is not None:
        return
    
    print(f"üîÑ Loading {model_name} model...")
    start_time = time.time()
    
    try:
        if WHISPER_TYPE == "openai":
            whisper_model = whisper.load_model(model_name)
        else:
            whisper_model = WhisperModel(model_name, device="cpu", compute_type="int8")
        
        load_time = time.time() - start_time
        print(f"‚úÖ {model_name} model loaded in {load_time:.2f}s")
        
    except Exception as e:
        print(f"‚ùå Failed to load model: {e}")
        raise

def generate_advanced_summary_with_ai(text, language="en"):
    """Generate comprehensive summary using your sophisticated AI analysis system"""
    
    if not text or len(text.strip()) < 50:
        fallback_responses = get_fallback_responses()
        return fallback_responses["summary_fallback"]
    
    # Truncate if needed
    text = truncate_transcript(text, max_length=8000)
    
    if API_PROVIDERS_AVAILABLE:
        try:
            # Use your sophisticated unified analysis prompt
            prompt = get_unified_analysis_prompt(text)
            print("ü§ñ Using advanced AI analysis with your prompting system...")
            
            # Try to call AI API using your multi-provider system
            response_text = call_api(prompt, max_tokens=8000)
            
            if response_text:
                try:
                    # Parse JSON response with comprehensive cleaning (same as complex engine)
                    if "```json" in response_text:
                        start = response_text.find("```json") + 7
                        end = response_text.find("```", start)
                        json_str = response_text[start:end].strip() if end > start else response_text[start:].strip()
                    elif "```" in response_text and "{" in response_text:
                        lines = response_text.split('\n')
                        json_lines = []
                        in_json = False
                        for line in lines:
                            if line.strip().startswith('{') or in_json:
                                in_json = True
                                json_lines.append(line)
                                if line.strip().endswith('}') and line.strip().count('{') <= line.strip().count('}'):
                                    break
                        json_str = '\n'.join(json_lines)
                    else:
                        start = response_text.find('{')
                        end = response_text.rfind('}') + 1
                        json_str = response_text[start:end] if start >= 0 and end > start else response_text
                    
                    # Comprehensive JSON cleaning (same as complex engine)
                    import re
                    json_str = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f-\x9f]', '', json_str)
                    json_str = json_str.replace('\n\n', '\\n').replace('\r', ' ').strip()
                    
                    # Parse and validate
                    ai_result = json.loads(json_str)
                    
                    # Validate required fields
                    required_fields = ["narrative_summary", "speaker_points", "enhanced_action_items", "key_decisions"]
                    for field in required_fields:
                        if field not in ai_result:
                            if field == "narrative_summary":
                                ai_result[field] = "AI-generated comprehensive summary"
                            else:
                                ai_result[field] = []
                    
                    print(f"‚úÖ Advanced AI analysis completed!")
                    print(f"   - Narrative summary: {len(ai_result.get('narrative_summary', ''))} chars")
                    print(f"   - Speaker points: {len(ai_result.get('speaker_points', []))} speakers")
                    print(f"   - Enhanced action items: {len(ai_result.get('enhanced_action_items', []))} items")
                    print(f"   - Key decisions: {len(ai_result.get('key_decisions', []))} decisions")
                    
                    return {
                        "summary": ai_result.get("narrative_summary", "AI-generated comprehensive summary"),
                        "key_decisions": ai_result.get("key_decisions", []),
                        "enhanced_action_items": ai_result.get("enhanced_action_items", []),
                        "speaker_points": ai_result.get("speaker_points", []),
                        "tags": ["ai-analysis", "advanced-prompting", "unified-analysis"],
                        "participants": [sp.get("speaker", "Speaker 1") for sp in ai_result.get("speaker_points", [])],
                        "meeting_type": "conversation",
                        "sentiment": "neutral",
                        "analysis_method": "advanced_ai_unified_analysis"
                    }
                    
                except json.JSONDecodeError as e:
                    print(f"‚ö†Ô∏è  JSON parsing failed: {e}")
                    print(f"üìÑ Response preview: {str(response_text)[:200]}...")
                except Exception as e:
                    print(f"‚ö†Ô∏è  Response processing failed: {e}")
                    pass
            
        except Exception as e:
            print(f"‚ö†Ô∏è  Advanced AI analysis failed: {e}")
    
    # Fallback: Use your sophisticated fallback responses
    print("üìù Using your sophisticated fallback responses...")
    fallback_responses = get_fallback_responses()
    return fallback_responses["summary_fallback"]

def format_compatible_response(transcription_result, file_info, job_id):
    """Format response to match complex engine format for frontend compatibility"""
    
    segments = transcription_result.get("segments", [])
    text = transcription_result.get("text", "")
    language = transcription_result.get("language", "en")
    
    # Format transcript segments (compatible with frontend)
    formatted_transcript = []
    for i, segment in enumerate(segments):
        formatted_segment = {
            "id": i,
            "start": segment.get("start", 0.0),
            "end": segment.get("end", 0.0),
            "text": segment.get("text", "").strip(),
            "speaker": "speaker-01",  # Simple: always single speaker
            "speaker_name": "Speaker 1",
            "confidence": 0.95,  # High confidence for simple engine
            "tags": [],
            "assigned_speaker": 1,
            "duration": segment.get("end", 0.0) - segment.get("start", 0.0),
            "words": segment.get("words", [])
        }
        formatted_transcript.append(formatted_segment)
    
    # Format transcript for AI analysis (same as complex engine)
    transcript_lines = []
    for segment in formatted_transcript:
        speaker = segment.get("speaker_name", "Speaker 1")
        text = segment.get("text", "").strip()
        if text:
            transcript_lines.append(f"{speaker}: {text}")
    
    formatted_text_for_ai = "\n".join(transcript_lines)
    
    # Use your sophisticated AI analysis system with formatted transcript
    print("ü§ñ Generating advanced summary using your sophisticated AI analysis...")
    ai_analysis = generate_advanced_summary_with_ai(formatted_text_for_ai, language)
    
    # Compatible response format with your advanced AI analysis
    compatible_response = {
        "filename": file_info["filename"],
        "job_id": job_id,
        "transcript": formatted_transcript,
        "summary": ai_analysis.get("summary", f"Fast transcription completed with {len(text.split())} words."),
        "key_decisions": ai_analysis.get("key_decisions", []),
        "tags": ai_analysis.get("tags", ["transcription", "fast-engine", "ai-analysis"]),
        "speakers": ai_analysis.get("participants", ["Speaker 1"]),
        "participants": ai_analysis.get("participants", ["Speaker 1"]),
        "meeting_type": ai_analysis.get("meeting_type", "general"),
        "sentiment": ai_analysis.get("sentiment", "neutral"),
        "duration": max([s.get("end", 0) for s in segments]) if segments else 0.0,
        "language": language,
        "word_count": len(text.split()) if text else 0,
        "audio_info": {
            "method": "fast_whisper_with_advanced_ai",
            "model": file_info.get("model_used", "tiny"),
            "model_version": WHISPER_TYPE,
            "sample_rate": 16000,
            "channels": 1,
            "processing_time": file_info.get("processing_time", 0),
            "total_segments": len(segments),
            "speaker_detection": "single_speaker_mode",
            "features_used": ["fast_transcription", "advanced_ai_analysis", "sophisticated_prompting"],
            "optimization_settings": {
                "beam_size": 1,
                "best_of": 1,
                "temperature": 0.0,
                "description": "Optimized for speed with advanced AI analysis"
            },
            "speed_mode": "fast",
            "ai_analysis_method": ai_analysis.get("analysis_method", "sophisticated_prompting_system")
        },
        "processed_at": datetime.now().isoformat(),
        "clean_summary": ai_analysis.get("summary", "Fast AI analysis completed"),
        "speaker_points": ai_analysis.get("speaker_points", [
            {
                "speaker": "Speaker 1",
                "points": [
                    f"Provided {len(text.split())} words of content",
                    "Single speaker conversation or monologue", 
                    "Content transcribed with high accuracy"
                ]
            }
        ]),
        "enhanced_action_items": ai_analysis.get("enhanced_action_items", [
            {
                "title": "Review Advanced AI Analysis",
                "description": "Review the AI-generated analysis that used your sophisticated prompting system for comprehensive insights.",
                "priority": "Medium",
                "category": "Review",
                "timeframe": "Immediate",
                "assigned_to": "User",
                "tags": ["review", "ai-analysis", "sophisticated-prompting"],
                "notion_ready": {
                    "title": "Review Advanced AI Analysis",
                    "properties": {
                        "Priority": "Medium",
                        "Category": "Review",
                        "Status": "Not Started",
                        "Source": "Advanced AI System"
                    }
                }
            }
        ]),
        "point_of_view": []
    }
    
    return compatible_response

@app.on_event("startup")
async def startup_event():
    """Load model on startup"""
    print("üöÄ Starting Fast Compatible Transcription Service...")
    load_simple_model("tiny")  # Start with fastest model
    print("‚úÖ Service ready!")

@app.get("/")
async def root():
    """Health check - compatible with existing frontend"""
    return {
        "service": "Fast AI Transcription",
        "status": "running",
        "whisper_type": WHISPER_TYPE,
        "model_loaded": whisper_model is not None,
        "compatibility": "full_compatibility_mode",
        "timestamp": datetime.now().isoformat()
    }

@app.get("/api/status")
async def get_status():
    """Status endpoint - compatible with existing frontend"""
    return {
        "models": {
            "faster_whisper": whisper_model is not None,
            "deepgram": False,  # Not available in simple mode
            "huggingface": False  # Not available in simple mode
        },
        "engines_available": ["simple-whisper"],
        "current_engine": "simple-whisper",
        "performance_mode": "optimized_for_speed"
    }

@app.post("/api/transcribe")
async def transcribe_audio(
    file: UploadFile = File(...),
    model: str = "tiny",
    language: str = "auto",
    transcription_engine: str = "simple-whisper"
):
    """Main transcription endpoint - fully compatible with existing frontend"""
    
    if model not in MODEL_OPTIONS:
        model = "tiny"  # Default to fastest
    
    # Create job ID (compatible format)
    job_id = f"job_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{os.getpid()}"
    
    try:
        # Save uploaded file
        uploads_dir = Path("uploads")
        uploads_dir.mkdir(exist_ok=True)
        
        original_file_path = uploads_dir / f"{job_id}_{file.filename}"
        
        print(f"üìÅ Processing: {file.filename}")
        with open(original_file_path, "wb") as buffer:
            content = await file.read()
            buffer.write(content)
        
        file_size_mb = len(content) / (1024 * 1024)
        print(f"üìè File size: {file_size_mb:.2f}MB")
        
        # Convert to MP3 if needed
        final_file_path = original_file_path
        if should_convert_to_mp3(file.filename):
            mp3_file_path = uploads_dir / f"{job_id}_converted.mp3"
            converted_path = convert_to_mp3(original_file_path, mp3_file_path)
            
            if converted_path != str(original_file_path):
                final_file_path = Path(converted_path)
                print(f"üéµ Using converted MP3")
        
        # Load model if needed
        load_simple_model(model)
        
        # Fast transcription
        print(f"üéµ Starting fast transcription...")
        start_time = time.time()
        
        if WHISPER_TYPE == "openai":
            result = whisper_model.transcribe(
                str(final_file_path),
                language=None if language == "auto" else language,
                fp16=False,
                verbose=False
            )
            
            text = result["text"].strip()
            detected_language = result["language"]
            segments = result.get("segments", [])
            
        else:  # faster-whisper
            segments_iter, info = whisper_model.transcribe(
                str(final_file_path),
                language=None if language == "auto" else language,
                beam_size=1,
                best_of=1,
                temperature=0.0,
                word_timestamps=True
            )
            
            text = ""
            segments = []
            
            for i, segment in enumerate(segments_iter):
                text += segment.text + " "
                segments.append({
                    "id": i,
                    "start": segment.start,
                    "end": segment.end,
                    "text": segment.text.strip(),
                    "words": [
                        {
                            "start": word.start,
                            "end": word.end,
                            "word": word.word,
                            "probability": getattr(word, 'probability', 0.9)
                        } for word in segment.words
                    ] if hasattr(segment, 'words') and segment.words else []
                })
                
                if i > 1000:  # Limit for performance
                    break
            
            text = text.strip()
            detected_language = info.language
        
        transcription_time = time.time() - start_time
        
        # Prepare file info
        file_info = {
            "filename": file.filename,
            "model_used": model,
            "processing_time": round(transcription_time, 2),
            "file_size_mb": round(file_size_mb, 2)
        }
        
        # Format compatible response
        transcription_result = {
            "text": text,
            "language": detected_language,
            "segments": segments
        }
        
        compatible_response = format_compatible_response(transcription_result, file_info, job_id)
        
        # Save result in compatible format
        results_dir = Path("results")
        results_dir.mkdir(exist_ok=True)
        
        result_path = results_dir / f"{job_id}_result.json"
        with open(result_path, "w", encoding="utf-8") as f:
            json.dump(compatible_response, f, ensure_ascii=False, indent=2)
        
        # Clean up files
        try:
            os.remove(original_file_path)
            if final_file_path != original_file_path:
                os.remove(final_file_path)
        except:
            pass
        
        # Performance info
        words_per_second = len(text.split()) / transcription_time if transcription_time > 0 else 0
        print(f"‚úÖ Fast transcription completed!")
        print(f"   Words: {len(text.split())}")
        print(f"   Time: {transcription_time:.2f}s")
        print(f"   Speed: {words_per_second:.1f} words/sec")
        print(f"   Result: {result_path}")
        
        return JSONResponse(content=compatible_response)
        
    except Exception as e:
        # Clean up on error
        try:
            if 'original_file_path' in locals():
                os.remove(original_file_path)
            if 'final_file_path' in locals() and final_file_path != original_file_path:
                os.remove(final_file_path)
        except:
            pass
        
        print(f"‚ùå Transcription failed: {e}")
        raise HTTPException(status_code=500, detail=f"Transcription failed: {str(e)}")

@app.get("/api/experimental-methods")
async def get_experimental_methods():
    """Get available experimental speaker detection methods"""
    return {
        "success": True,
        "experimental_methods": {
            "enhanced": {
                "name": "Enhanced Pattern Detection",
                "description": "AI-powered conversation analysis (optimized)",
                "accuracy": "high",
                "speed": "ultra_fast"
            },
            "simple": {
                "name": "Simple Speaker Detection",  
                "description": "Fast pattern-based detection",
                "accuracy": "medium",
                "speed": "ultra_fast"
            }
        },
        "default_method": "enhanced",
        "description": "Fast speaker detection methods optimized for speed"
    }

@app.get("/api/jobs/completed")
async def get_completed_jobs():
    """Get list of completed jobs with basic info"""
    results_dir = os.path.join(os.path.dirname(__file__), "results")
    if not os.path.exists(results_dir):
        return {"jobs": []}
    
    completed_jobs = []
    for filename in os.listdir(results_dir):
        if filename.endswith('_result.json'):
            job_id = filename.replace('_result.json', '')
            result_file = os.path.join(results_dir, filename)
            
            try:
                with open(result_file, 'r', encoding='utf-8') as f:
                    result = json.load(f)
                
                completed_jobs.append({
                    "job_id": job_id,
                    "filename": result.get('filename', 'Unknown'),
                    "duration": result.get('duration', 0),
                    "word_count": result.get('word_count', 0),
                    "processed_at": result.get('processed_at', ''),
                    "summary_preview": result.get('summary', '')[:100] + "..." if result.get('summary') else ""
                })
            except Exception as e:
                print(f"Error reading result file {filename}: {e}")
                continue
    
    # Sort by processed_at descending
    completed_jobs.sort(key=lambda x: x['processed_at'], reverse=True)
    
    return {"jobs": completed_jobs}

@app.get("/api/status")
async def get_api_status():
    """Get API status with engine information"""
    return {
        "service": "Fast AI Transcription",
        "status": "running",
        "whisper_type": WHISPER_TYPE,
        "model_loaded": True,
        "compatibility": "full_compatibility_mode",
        "engines_available": ["simple-whisper"],
        "performance_mode": "optimized_for_speed",
        "optimization_applied": True
    }

@app.get("/api/jobs/{job_id}/result")
async def get_job_result(job_id: str):
    """Get full result data for a completed job"""
    results_dir = os.path.join(os.path.dirname(__file__), "results")
    result_file = os.path.join(results_dir, f"{job_id}_result.json")
    
    if not os.path.exists(result_file):
        raise HTTPException(status_code=404, detail="Job result not found")
    
    try:
        with open(result_file, 'r', encoding='utf-8') as f:
            result = json.load(f)
        
        return {
            "success": True,
            "job_id": job_id,
            "result": result
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error reading result file: {str(e)}")

@app.get("/api/result/{job_id}")
async def get_result_direct(job_id: str):
    """Direct result access - compatibility endpoint"""
    results_dir = os.path.join(os.path.dirname(__file__), "results")
    result_file = os.path.join(results_dir, f"{job_id}_result.json")
    
    if not os.path.exists(result_file):
        raise HTTPException(status_code=404, detail="Result file not found")
    
    try:
        with open(result_file, 'r', encoding='utf-8') as f:
            result = json.load(f)
        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error reading result file: {str(e)}")

@app.get("/api/audio/{job_id}")
async def get_audio_file(job_id: str):
    """Serve processed audio file for playback"""
    try:
        uploads_dir = os.path.join(os.path.dirname(__file__), "uploads")
        
        # Look for processed audio file first
        processed_file = os.path.join(uploads_dir, f"{job_id}_processed.wav")
        if os.path.exists(processed_file):
            from fastapi.responses import FileResponse
            return FileResponse(
                processed_file,
                media_type="audio/wav",
                headers={"Content-Disposition": f"inline; filename={job_id}_processed.wav"}
            )
        
        # Fall back to original file
        for ext in ['.wav', '.mp3', '.m4a', '.mp4', '.webm', '.mkv', '.flac', '.ogg', '.mov']:
            original_file = os.path.join(uploads_dir, f"{job_id}{ext}")
            if os.path.exists(original_file):
                from fastapi.responses import FileResponse
                media_type = f"audio/{ext[1:]}" if ext != '.webm' else "audio/webm"
                return FileResponse(
                    original_file,
                    media_type=media_type,
                    headers={"Content-Disposition": f"inline; filename={job_id}{ext}"}
                )
        
        raise HTTPException(status_code=404, detail=f"Audio file not found for job_id: {job_id}")
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")

from pydantic import BaseModel
from typing import Optional, List

# Chat session storage
chat_sessions = {}

class ChatRequest(BaseModel):
    query: str
    session_id: Optional[str] = None
    file_id: Optional[str] = None
    model_preference: Optional[str] = "faiss"  # Default to FAISS offline
    use_smart_routing: Optional[bool] = True  # Enable smart model routing

class ChatResponse(BaseModel):
    response: str
    sources: List[dict] = []
    session_id: str
    timestamp: str
    confidence: float

@app.post("/api/chat/load/{job_id}")
@app.get("/api/chat/load/{job_id}")
async def load_chat_data(job_id: str):
    """Load transcript data for specific job into chat system"""
    try:
        # Check if result file exists
        results_dir = os.path.join(os.path.dirname(__file__), "results")
        result_file = os.path.join(results_dir, f"{job_id}_result.json")
        
        if os.path.exists(result_file):
            # Load and store transcript data in session
            with open(result_file, 'r', encoding='utf-8') as f:
                transcript_data = json.load(f)
            
            # Store in global chat sessions for easy access
            chat_sessions[job_id] = {
                "transcript_data": transcript_data,
                "loaded_at": datetime.now().isoformat(),
                "conversation_history": []
            }
            
            print(f"üìö Chat data loaded for job {job_id}: {transcript_data.get('filename', 'Unknown')}")
            
            return {
                "status": "success", 
                "message": f"Chat system loaded for job {job_id}",
                "job_id": job_id,
                "chat_available": True,
                "data_loaded": True,
                "transcript_info": {
                    "filename": transcript_data.get("filename", "Unknown"),
                    "duration": transcript_data.get("duration", 0),
                    "word_count": transcript_data.get("word_count", 0),
                    "speakers": len(transcript_data.get("speakers", [])),
                    "segments": len(transcript_data.get("transcript", []))
                }
            }
        else:
            return {
                "status": "error",
                "message": f"No data found for job {job_id}",
                "job_id": job_id,
                "chat_available": False,
                "data_loaded": False
            }
    except Exception as e:
        print(f"‚ùå Chat load error: {e}")
        return {
            "status": "error",
            "message": f"Failed to load data: {str(e)}",
            "job_id": job_id,
            "chat_available": False,
            "data_loaded": False
        }

@app.post("/api/chat/enhanced")
async def chat_enhanced(request: ChatRequest):
    """Enhanced chat with AI system that can answer questions about transcripts"""
    try:
        transcript_data = None
        session_id = request.session_id or "default"
        model_preference = getattr(request, 'model_preference', 'faiss')
        
        # Try to get transcript data from multiple sources
        if request.file_id:
            # First priority: file_id parameter
            if request.file_id in chat_sessions:
                transcript_data = chat_sessions[request.file_id]["transcript_data"]
                print(f"üí¨ Using loaded transcript data for {request.file_id}")
            else:
                # Try to load from file
                results_dir = os.path.join(os.path.dirname(__file__), "results")
                result_file = os.path.join(results_dir, f"{request.file_id}_result.json")
                if os.path.exists(result_file):
                    with open(result_file, 'r', encoding='utf-8') as f:
                        transcript_data = json.load(f)
                    print(f"üí¨ Loaded transcript data from file for {request.file_id}")
        
        # If no specific file_id, try to find most recent loaded session
        if not transcript_data and chat_sessions:
            # Use the most recently loaded transcript
            most_recent_job = max(chat_sessions.keys(), key=lambda k: chat_sessions[k]["loaded_at"])
            transcript_data = chat_sessions[most_recent_job]["transcript_data"]
            request.file_id = most_recent_job
            print(f"üí¨ Using most recent transcript: {most_recent_job}")
        
        if transcript_data:
            # Use Multi-Model Chat System (preferred) for intelligent routing
            if MULTI_MODEL_CHAT_AVAILABLE and multi_model_chat:
                print(f"üöÄ Using Multi-Model Chat System for: {request.query}")
                
                # Load transcript data into multi-model system
                results_dir = os.path.join(os.path.dirname(__file__), "results")
                result_file = os.path.join(results_dir, f"{request.file_id}_result.json")
                
                if multi_model_chat.load_transcription_data(result_file):
                    # Use smart query with model preference
                    use_smart_routing = getattr(request, 'use_smart_routing', True)
                    multi_response = multi_model_chat.smart_query(
                        query=request.query,
                        session_id=session_id,
                        model_preference=model_preference,
                        use_smart_routing=use_smart_routing
                    )
                    
                    # Store conversation in session history
                    if request.file_id in chat_sessions:
                        chat_sessions[request.file_id]["conversation_history"].append({
                            "query": request.query,
                            "response": multi_response["response"],
                            "model_used": multi_response.get("model_used", "multi_model"),
                            "timestamp": datetime.now().isoformat()
                        })
                    
                    return ChatResponse(
                        response=multi_response["response"],
                        sources=[{
                            "type": "multi_model_system", 
                            "job_id": request.file_id,
                            "filename": transcript_data.get("filename", "Unknown"),
                            "model_used": multi_response.get("model_used", "multi_model"),
                            "confidence": multi_response.get("confidence", 0.0)
                        }] + multi_response.get("sources", []),
                        session_id=session_id,
                        timestamp=datetime.now().isoformat(),
                        confidence=multi_response.get("confidence", 0.9)
                    )
                else:
                    print("‚ö†Ô∏è Failed to load data into Multi-Model system, using fallback")
            
            # Fallback 1: Use FAISS directly for 'faiss' preference
            elif model_preference == 'faiss' and FAISS_CHAT_AVAILABLE and faiss_chat and faiss_chat.is_available():
                print(f"üîã Using FAISS offline chat system for: {request.query}")
                
                # Load transcript data into FAISS if not already loaded
                results_dir = os.path.join(os.path.dirname(__file__), "results")
                result_file = os.path.join(results_dir, f"{request.file_id}_result.json")
                
                if faiss_chat.load_transcription_data(result_file):
                    faiss_response = faiss_chat.query(request.query, session_id)
                    
                    # Store conversation in session history
                    if request.file_id in chat_sessions:
                        chat_sessions[request.file_id]["conversation_history"].append({
                            "query": request.query,
                            "response": faiss_response["response"],
                            "model_used": "faiss_offline",
                            "timestamp": datetime.now().isoformat()
                        })
                    
                    return ChatResponse(
                        response=faiss_response["response"],
                        sources=[{
                            "type": "faiss_offline", 
                            "job_id": request.file_id,
                            "filename": transcript_data.get("filename", "Unknown"),
                            "segments_found": faiss_response.get("segments_found", 0),
                            "confidence": faiss_response.get("confidence", 0.0)
                        }],
                        session_id=session_id,
                        timestamp=datetime.now().isoformat(),
                        confidence=faiss_response.get("confidence", 0.9)
                    )
                else:
                    print("‚ö†Ô∏è Failed to load data into FAISS, falling back to online AI")
            
            # Fallback 2: Original online AI system (Mistral/others)
            print(f"üß† Using online AI system for: {request.query}")
            response_text = await generate_chat_response(request.query, transcript_data, session_id)
            
            # Store conversation in session history
            if request.file_id in chat_sessions:
                chat_sessions[request.file_id]["conversation_history"].append({
                    "query": request.query,
                    "response": response_text,
                    "model_used": "online_ai",
                    "timestamp": datetime.now().isoformat()
                })
            
            return ChatResponse(
                response=response_text,
                sources=[{
                    "type": "transcript", 
                    "job_id": request.file_id,
                    "filename": transcript_data.get("filename", "Unknown")
                }],
                session_id=session_id,
                timestamp=datetime.now().isoformat(),
                confidence=0.9
            )
        else:
            # No transcript data available - generic response
            response_text = await generate_generic_chat_response(request.query)
            
            return ChatResponse(
                response=f"No transcript is currently loaded. {response_text}\n\nTo chat about a specific transcript, please load it first using the 'Load Chat Data' button on a transcript result.",
                sources=[],
                session_id=session_id,
                timestamp=datetime.now().isoformat(),
                confidence=0.6
            )
        
    except Exception as e:
        print(f"‚ùå Chat error: {e}")
        return ChatResponse(
            response="I apologize, but I'm having trouble processing your request right now. Please try again or make sure a transcript is loaded first.",
            sources=[],
            session_id=request.session_id or "default",
            timestamp=datetime.now().isoformat(),
            confidence=0.3
        )

async def generate_chat_response(query: str, transcript_data: dict, session_id: str = "default") -> str:
    """Generate AI response based on query and transcript data with full context"""
    try:
        # Extract comprehensive information from transcript
        transcript_segments = transcript_data.get("transcript", [])
        summary = transcript_data.get("summary", "")
        key_decisions = transcript_data.get("key_decisions", [])
        enhanced_action_items = transcript_data.get("enhanced_action_items", [])
        speakers = transcript_data.get("speakers", [])
        filename = transcript_data.get("filename", "Unknown")
        duration = transcript_data.get("duration", 0)
        word_count = transcript_data.get("word_count", 0)
        
        # Format full transcript for AI context (not just first 20 segments)
        transcript_text = ""
        for i, segment in enumerate(transcript_segments):
            if i >= 50:  # Limit to first 50 segments for context window
                transcript_text += f"\n... (transcript continues with {len(transcript_segments) - 50} more segments)"
                break
            speaker = segment.get("speaker_name", "Speaker")
            text = segment.get("text", "")
            start_time = segment.get("start", 0)
            timestamp = f"{int(start_time//60):02d}:{int(start_time%60):02d}"
            transcript_text += f"[{timestamp}] {speaker}: {text}\n"
        
        # Format action items
        action_items_text = ""
        for item in enhanced_action_items[:10]:
            if isinstance(item, dict):
                title = item.get("title", "Action item")
                description = item.get("description", "")
                action_items_text += f"‚Ä¢ {title}: {description}\n"
            else:
                action_items_text += f"‚Ä¢ {item}\n"
        
        # Create comprehensive context-aware prompt
        prompt = f"""You are an AI assistant analyzing a meeting transcript. Here's the complete context:

MEETING INFORMATION:
- File: {filename}
- Duration: {duration/60:.1f} minutes
- Word Count: {word_count} words
- Speakers: {', '.join(speakers)}

MEETING SUMMARY:
{summary}

KEY DECISIONS:
{chr(10).join([f"‚Ä¢ {decision}" for decision in key_decisions[:10]])}

ACTION ITEMS:
{action_items_text}

FULL TRANSCRIPT:
{transcript_text}

USER QUESTION: {query}

Please provide a helpful, detailed, and accurate answer based on the transcript content. Use specific quotes and timestamps when relevant. If you need to reference speakers, use their names. If the information isn't available in the transcript, please say so clearly."""

        # Use our AI system
        api_providers = initialize_providers()
        if api_providers:
            response = call_api(prompt, providers=api_providers, max_tokens=1200)
            return response.strip()
        else:
            # Fallback response with available information
            return f"""I can see the transcript data for "{filename}" ({duration/60:.1f} minutes, {word_count} words) with {len(speakers)} speakers: {', '.join(speakers)}.

However, I'm currently unable to process AI analysis. Here's what I can tell you from the available data:

**Summary**: {summary[:300]}...

**Key Decisions**: {chr(10).join([f"‚Ä¢ {decision}" for decision in key_decisions[:3]])}

You can ask me to search for specific information in the transcript or review the full data."""
            
    except Exception as e:
        print(f"‚ùå Chat response generation error: {e}")
        return f"""I found the transcript for "{transcript_data.get('filename', 'Unknown')}" but I'm having trouble analyzing it right now. 

The transcript contains:
‚Ä¢ {len(transcript_data.get('transcript', []))} segments
‚Ä¢ {len(transcript_data.get('speakers', []))} speakers
‚Ä¢ {transcript_data.get('word_count', 0)} words

You can review the summary and segments directly, or try asking a more specific question."""

async def generate_generic_chat_response(query: str) -> str:
    """Generate generic AI response without specific transcript context"""
    try:
        prompt = f"""You are an AI assistant specialized in meeting transcription and analysis. Please respond to this question: {query}

Provide helpful information about transcription, meeting analysis, or general questions. Be concise and helpful."""

        api_providers = initialize_providers()
        if api_providers:
            response = call_api(prompt, providers=api_providers, max_tokens=400)
            return response.strip()
        else:
            return "I'm an AI assistant that can help you with transcription analysis. However, AI services are currently limited. I can help you understand how to use the transcription features and analyze your meeting data."
            
    except Exception as e:
        print(f"‚ùå Generic chat response error: {e}")
        return "I'm here to help with your transcription analysis needs. Feel free to ask about specific transcripts, summaries, or how to use the system features."

@app.get("/api/chat/sessions")
async def get_chat_sessions():
    """Get information about loaded chat sessions"""
    return {
        "sessions": {
            job_id: {
                "filename": session["transcript_data"].get("filename", "Unknown"),
                "loaded_at": session["loaded_at"],
                "conversation_count": len(session["conversation_history"]),
                "transcript_info": {
                    "duration": session["transcript_data"].get("duration", 0),
                    "word_count": session["transcript_data"].get("word_count", 0),
                    "speakers": len(session["transcript_data"].get("speakers", []))
                }
            }
            for job_id, session in chat_sessions.items()
        },
        "total_sessions": len(chat_sessions)
    }

@app.get("/api/chat/history/{job_id}")
async def get_chat_history(job_id: str):
    """Get conversation history for a specific job"""
    if job_id in chat_sessions:
        return {
            "job_id": job_id,
            "filename": chat_sessions[job_id]["transcript_data"].get("filename", "Unknown"),
            "conversation_history": chat_sessions[job_id]["conversation_history"],
            "loaded_at": chat_sessions[job_id]["loaded_at"]
        }
    else:
        raise HTTPException(status_code=404, detail="Chat session not found")

@app.get("/api/chat/suggestions")
async def get_chat_suggestions():
    """Get suggested questions for current transcript"""
    if MULTI_MODEL_CHAT_AVAILABLE and multi_model_chat:
        # Use multi-model system for suggestions
        try:
            suggestions = multi_model_chat.get_suggestions()
            return {
                "suggestions": suggestions,
                "source": "multi_model_system"
            }
        except:
            pass
    
    if FAISS_CHAT_AVAILABLE and faiss_chat and faiss_chat.is_available():
        suggestions = faiss_chat.get_suggestions()
        return {
            "suggestions": suggestions,
            "source": "faiss_offline"
        }
    else:
        # Default suggestions
        return {
            "suggestions": [
                "What is this transcript about?",
                "Who are the main speakers?", 
                "What are the key topics discussed?",
                "Can you summarize the main points?",
                "What decisions were made?",
                "What action items were mentioned?"
            ],
            "source": "default"
        }

@app.get("/api/chat/faiss/stats")
async def get_faiss_stats():
    """Get FAISS system statistics"""
    if FAISS_CHAT_AVAILABLE and faiss_chat:
        stats = faiss_chat.get_index_stats()
        return {
            "faiss_available": faiss_chat.is_available(),
            "stats": stats
        }
    else:
        return {
            "faiss_available": False,
            "stats": {"status": "unavailable", "message": "FAISS chat system not installed"}
        }

@app.get("/api/chat/models")
async def get_available_models():
    """Get information about available chat models"""
    models = {}
    
    if MULTI_MODEL_CHAT_AVAILABLE and multi_model_chat:
        # Get models from multi-model system
        models.update(multi_model_chat.get_available_models())
        return {
            "models": models,
            "default_model": "faiss",
            "smart_routing_available": True,
            "source": "multi_model_system"
        }
    
    # Fallback: individual systems
    if FAISS_CHAT_AVAILABLE and faiss_chat and faiss_chat.is_available():
        models["faiss"] = {
            "name": "FAISS Offline Search",
            "capabilities": ["offline_search", "semantic_similarity"],
            "offline": True,
            "priority": 0
        }
    
    models["mistral"] = {
        "name": "Mistral AI (Online)",
        "capabilities": ["advanced_analysis", "chat"],
        "offline": False,
        "priority": 1
    }
    
    return {
        "models": models,
        "default_model": "faiss" if "faiss" in models else "mistral",
        "smart_routing_available": False,
        "source": "individual_systems"
    }

if __name__ == "__main__":
    import uvicorn
    print("üöÄ Starting Fast Compatible Transcription Server on port 8000")
    uvicorn.run(app, host="0.0.0.0", port=8000)
